---
title: "SummativeAssessmentSectionB"
author: "Kexin Wu"
date: "01/01/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
```

## Section B (30 marks)

### B.1

In this question we consider a security system at a factory. A sensor is designed to make a sound if a person walks within one metre of the gate. However, the sensor is not perfectly reliable: It sometimes makes a sound when there is no one present, and sometimes fails to make a sound when someone is present.

For simplicity we will view the passage of time as being broken down into a series of phases lasting exactly one minute. For each minute, we let $p_0$ denote the conditional probability that the sensor makes a sound if there is no person within one metre of the gate, during that minute. Moreover, for each minute, we let $p_1$ denote the conditional probability that the sensor makes a sound at least once, if there is at least one person present, during that minute. Suppose also that the probability that at least one person walks within one metre of the gate over any given minute is $q$. Again, for simplicity, we assume that $p_0$, $p_1$, $q \in [0, 1]$ are all constant. Let $\phi$ denote the conditional probability that at least one person has passed within one metre of the gate during the current minute, given that the alarm has made a sound during that minute.

**(a.Q)**  Write a function called c_prob_person_given_alarm which gives $\phi$ as a function of $p_0$, $p_1$ and $q$.

**(a.A)**Let A be the event that the sensor makes a sound, and B be the event that at least one person is within one metre of the gate during that minute.

So we have $p_0 = \mathbb{P}(A \mid B^C)$, $p_1 = \mathbb{P}(A \mid B)$, $q = \mathbb{P}(B)$, and $\phi = \mathbb{P}(B \mid A)$.

First, we compute the $\mathbb{P}(A)$:
$$
\mathbb{P}(A) = \mathbb{P}(A \cap B) + \mathbb{P}(A \cap B^C) = \mathbb{P}(A \mid B) \cdot \mathbb{P}(B) + \mathbb{P}(A \mid B^C) \cdot \mathbb{P}(B^C) = p_1 \cdot q + p_0 \cdot (1-q)
$$

Then, from the Bayes theorm, we have 
$$
\phi = \mathbb{P}(B \mid A) = \frac{\mathbb{P}(B) \cdot \mathbb{P}(A \mid B)}{\mathbb{P}(A)} = \frac{q \cdot p_1}{p_1 \cdot q + p_0 \cdot (1 - q)}
$$


```{r}
c_prob_person_given_alarm <- function(p0, p1, q){
  phi = q * p1 / (p1 * q + p0 * (1 - q))
  return(phi)
}
```

**(b.Q)** Consider a setting in which $p_0 = 0.05$, $p_1 = 0.95$ and $q = 0.1$. In this case, what is $\phi$?

**(b.A)**
```{r}
p0 <- 0.05
p1 <- 0.95
q <- 0.1
phi <- c_prob_person_given_alarm(p0, p1, q)
phi
```

**(c.Q)** Next consider a setting in which $p_0 = 0.05$, $p_1 = 0.95$ and generate a plot which shows $\phi$ as we vary $q$. That is, you should display a curve which has $q$ along the horizontal axis and the corresponding value of $\phi$ along the vertical axis.

**(c.A)**
```{r}
library(latex2exp)

p0 <- 0.05
p1 <- 0.95

phi <- data.frame(q = seq(0, 1, 0.0001))
phi %>%
  mutate(phi = c_prob_person_given_alarm(p0, p1, q)) %>%
  ggplot(mapping = aes(x = q, y = phi)) + 
  geom_line() +
  theme_bw() +
  xlab("q") + 
  ylab(TeX("$\\phi$"))
```

----------------

### B.2

Suppose that $\alpha, \beta, \gamma \in [0, 1]$ with $\alpha + \beta + \gamma \leq 1$ and let $X$ be a discrete random variable with with distribution supported on $\{0, 1, 2, 5\}$. Suppose that $\mathbb{P}(X = 1) = \alpha$, $\mathbb{P}(X = 2) = \beta$, $\mathbb{P}(X = 5) = \gamma$ and $\mathbb{P}(X \notin \{0, 1, 2, 5\}) = 0$.

**(a.Q)** What is the probability mass function $p_X: \mathbb{R} \to [0, 1]$ for $X$?

**(a.A)**
The probability mass function $p_X: \mathbb{R} \to [0, 1]$ for $X$ is:
$$
p_X(x) = \begin{cases}
1- \alpha - \beta - \gamma & if \quad x = 0 \\
\alpha & if \quad x = 1 \\
\beta & if \quad x = 2 \\
\gamma & if \quad x = 5
\end{cases} 
$$


**(b.Q)** Give an expression for the expectation of $X$ in terms of $\alpha, \beta, \gamma$.

**(b.A)** The expression for the expectation of $X$ is:
$$
\mathbb{E}(X) = (1 - \alpha - \beta - \gamma) \cdot 0 + \alpha \cdot 1 + \beta \cdot 2 + \gamma \cdot 5 = \alpha + 2\beta + 5\gamma
$$

**(c.Q)** Give an expression for the population variance of $X$ in terms of $\alpha, \beta, \gamma$.

**(c.A)** The expression for the population variance of $X$ is:
$$
Var(X) = \mathbb{E}[X^2] - \mathbb{E}(X)^2 = (\alpha + 4\beta + 25\gamma) - (\alpha + 2\beta + 5\gamma)^2 = \alpha + 4\beta + 25\gamma -\alpha^2 - 4\beta^2 - 25\gamma^2 - 4\alpha \beta - 10\alpha \gamma - 20\beta \gamma
$$

----------

Suppose $X_1, . . . , X_n$ is a sample consisting of independent and identically distributed random variables with $\mathbb{P}(X_i = 1) = \alpha$, $\mathbb{P}(X_i = 2) = \beta$, $\mathbb{P}(X_i = 5) = \gamma$ and $\mathbb{P}(X_i \notin \{0, 1, 2, 5\})=0$ for $i = 1,...,n$. Let $\overline{X} := \frac{1}{n} \sum_{i = 1}^{n}X_i$ be the sample mean.

**(d.Q)** Give an expression for the expectation of the random variable $\overline{X}$ in terms of $\alpha, \beta, \gamma$.

**(d.A)** The expectation of the random variable $\overline{X}$ is:
$$
\mathbb{E}(\overline{X}) = \alpha + 2\beta + 5\gamma
$$

**(e.Q)** Give an expression for the population variance of the random variable $\overline{X}$ in terms of $\alpha, \beta, \gamma, n$.

**(e.A)** The population variance of the random variable $\overline{X}$ is:
$$
Var(\overline{X}) = \frac{\alpha + 4\beta + 25\gamma -\alpha^2 - 4\beta^2 - 25\gamma^2 - 4\alpha \beta - 10\alpha \gamma - 20\beta \gamma}{n}
$$

**(f.Q)** Create a function called sample_X_0125() which takes as inputs $\alpha, \beta, \gamma$ and $n$ and outputs a sample $X_1,...,X_n$ of independent copies of $X$ where $\mathbb{P}(X = 1) = \alpha$, $\mathbb{P}(X = 2) = \beta$, $\mathbb{P}(X = 5) = \gamma$ and $P(X \notin \{0,1,2,5\}) = 0$.

**(f.A)**
```{r}
sample_X_0125 <- function(alpha, beta, gamma, n){
  sample_X <- data.frame(U = runif(n)) %>%
    mutate(X = case_when(
      (0 <= U) & (U < alpha) ~ 1,
      (alpha <= U) & (U < alpha + beta) ~ 2,
      (alpha + beta <= U) & (U < alpha + beta + gamma) ~ 5,
      (alpha + beta + gamma <= U) & (U <= 1) ~ 0)) %>%
  pull(X)
}
```

**(g.Q)** Suppose that $\alpha = 0.1$, $\beta = 0.2$, $\gamma = 0.3$. Use your function to generate a sample of size $n = 100000$ consisting of independent copies of $X$ where $\mathbb{P}(X = 1) = \alpha$, $\mathbb{P}(X = 2) = \beta$, $\mathbb{P}(X = 5) = \gamma$ and $\mathbb{P} (X \notin \{0, 1, 2, 5\}) = 0$. What value do you observe for $X$? What value do you observe for the sample variance? Is this the type of result you expect? Explain your answer.

**(g.A)** 
```{r}
n <- 100000
alpha <- 0.1
beta <- 0.2
gamma <- 0.3

sample_X <- sample_X_0125(alpha, beta, gamma, n)
mean(sample_X)
var(sample_X)
```

Based on the answer of question in section d and f, we have 
$$
\mathbb{E}(X) = \alpha + 2\beta + 5\gamma = 2
$$ 
and 
$$
Var(X) = \alpha + 4\beta + 25\gamma -\alpha^2 - 4\beta^2 - 25\gamma^2 - 4\alpha \beta - 10\alpha \gamma - 20\beta \gamma = 4.4
$$
Hence, the $\overline{X}$ is close to the expectation of $X$ and the sample variance is close to the population variance.

**(h.Q)** Once again, take $\alpha = 0.1$, $\beta = 0.2$, $\gamma = 0.3$. Conduct a simulation study to explore the behavior of the sample mean. Your study should involve $10000$ trials. In each trial, you should set $n = 100$ and create a sample $X_1,...,X_n$ of independent and identically distributed random variables with $\mathbb{P}(X = 1) = \alpha$, $\mathbb{P}(X = 2) = \beta$, $\mathbb{P}(X = 5) = \gamma$ and $\mathbb{P} (X \notin \{0, 1, 2, 5\}) = 0$. for $i = 1,...,n$. For each of the $10000$ trials, compute the corresponding sample mean $\overline{X}$ based on $X_1, . . . , X_n$.

**(h.A)** 
```{r}
set.seed(0)
num_trials <- 10000
alpha <- 0.1
beta <- 0.2
gamma <- 0.3
n <- 100

simulation_X <- data.frame(trial = 1: num_trials) %>%
  mutate(sample_X = map(.x = trial, ~sample_X_0125(alpha, beta, gamma, n))) %>% 
  mutate(sample_avg = map_dbl(.x = sample_X, ~mean(.x)))

```

**(i.Q)** Generate a histogram plot which displays the behavior of the sample mean within your simulation study. Use a bin width of $0.02$. The height of each bar should correspond to the number of times the sample mean took on a value within the corresponding bin.

**(i.A)** 
```{r}
ggplot(simulation_X, aes(x = sample_avg)) + 
  xlab("Sample Average") + 
  geom_histogram(binwidth = 0.02) + 
  ylab("Count")
```

**(j.Q)** What is the numerical value of the expectation $\mathbb{E}(\overline{X})$ in your simulation study? What is the numerical value of the variance $Var(\overline{X})$? Give your answers to 4 decimal places.

**(j.A)**
```{r}
sample_expection <- simulation_X %>%
  pull(sample_avg) %>%
  mean(na.rm = TRUE)

sample_expection %>% round(digits = 4)

sample_variance <- simulation_X %>%
  pull(sample_avg) %>%
  var(na.rm = TRUE)

sample_variance %>% round(digits = 4)
```


Let $f_{\mu, \sigma}: \mathbb{R} \to [0, \infty)$ be the probability density function of a Gaussian random variable with distribution $\mathcal{N}(\mu, \sigma^2)$, so that the population mean is $\mu$ and the population variance is $\sigma^2$.

**(k.Q)** Now append to your histogram plot an additional curve of the form $x \mapsto 200 \cdot f_{\mu, \sigma}(x)$, which displays a rescaled version of the probability density function of a Gaussian random variable with population mean $\mu = \mathbb{E}(\overline{X})$ and population variance $\mu^2 = Var(\overline{X})$. You may wish to consider your rescaled density $x \mapsto 200 \cdot f_{\mu, \sigma}(x)$ displayed for a sequence of $x$-values between $\mu − 4 \cdot \sigma$ and $\mu + 4\sigma$ in increments of $0.0001$. Make sure that the plot is well-presented and both the histogram and the rescaled density are clearly visible.

**(k.A)**
```{r}
mu <- sample_expection
sigma <- sample_variance
sd <- sqrt(sigma)
increment <- 0.0001
x <- seq(mu - 4 * sd, mu + 4 * sd, increment)

# create the Gaussian random distribution
gaussian_sample_X <- data.frame(x = x, density = 200 * dnorm(x, mean = mu, sd = sd), var = sigma)

colors <- c("Gaussian sample" = "red", "Histogram sample" = "blue")

fill <- c("Gaussian sample" = "white", "Histogram sample" = "white")

ggplot() + labs(x = "x", y = "Count") + theme_bw() + 
  geom_line(data = gaussian_sample_X, 
            aes(x, y = density, color = "Gaussian sample"), size = 1) +
  # create plot of Gaussian density
  geom_histogram(data = simulation_X, aes(x = sample_avg, color = "Histogram sample", fill = "Histogram sample"), binwidth = 0.02) +
  scale_color_manual(name = "Legend", values = colors) + 
  scale_fill_manual(name = "Legend", values = fill) 
```

**(k.Q)** Discuss the relationship between the histogram and the additional curve you observe. Can you explain what you observe?

**(k.A)**


-----------------

### B.3

In this question we shall use the exponential distribution to model time intervals between arrival times of birds at a bird feeder.

Let $\lambda > 0$ be a positive real number. An exponential random variable $X$ with parameter $\lambda$ is a continuous random variable with density $p_\lambda(x): \mathbb{R} \to (0, \infty)$ defined by
$$
p_\lambda(x) := \begin{cases}
0 & if \quad x < 0 \\
\lambda e^{-\lambda e} & if \quad x \geq 0
\end{cases}
$$

**(a.Q)** Give a formula for the the population mean and variance of an exponential random variable $X$ with parameter $\lambda$.

**(a.A)** Using integration by parts we have,
$$
\mathbb{E}[X] = \int_{-\infty}^{\infty}xp_\lambda(x)dx = \int_0^\infty\lambda xe^{-\lambda x}dx
= [x e^{-\lambda x}]_0^\infty + \int_0^\infty e^{-\lambda x}dx 
= 0 + [-\frac{1}{\lambda}e^{-\lambda x}]_0^\infty = \frac{1}{\lambda}
$$

Using integration by parts again we have,
$$
\mathbb{E}[X^2] = \int_{-\infty}^{\infty}xp_\lambda(x) dx = \int_0^\infty \lambda x^2 e^{-\lambda x}dx 
= [-x^2 e^{-\lambda x}]_0^\infty + 2\int_0^\infty xe^{-\lambda x}dx = \frac{2}{\lambda} \int_0^\infty \lambda x e^{-\lambda x}dx = \frac{2}{\lambda} \mathbb{E}[X] = \frac{2}{\lambda^2}
$$
Hence, $Var(X) = \mathbb{E}[X^2] - \mathbb{E}[X]^2 = \frac{2}{\lambda ^2} - \frac{1}{\lambda ^2} = \frac{1}{\lambda ^2}$.

**(b.Q)** Give a formula for the cumulative distribution function and the quantile function for exponential random variables with parameter $\lambda$.

**(b.A)** The cumulative distribution function is given by 
$$
F_\lambda(x) = \in
t_{-\infty}^xp_\lambda(t)dt 
= \begin{cases}
0 & if \quad x < 0 \\
\int_0^x\lambda e^{-\lambda t}dt & if \quad x \geq 0
\end{cases}
$$

Moreover, we have
$$
\int_0^x\lambda e^{-\lambda t}dt = \lambda \int_0^x e^{-\lambda t}dt = [-e^{-\lambda t}]_0^x = 1 - e^{-\lambda x}
$$

Thus, the cumulative distribution function is given by 
$$
F_\lambda(x) = \begin{cases}
0 & if \quad x < 0 \\
1 - e^{-\lambda x} & if \quad x \geq 0
\end{cases}
$$

The quantile function is given by 
$$
F^{-1}_\lambda (p) := inf\{x \in R: F_\lambda(x) \leq p\}
= \begin{cases}
-\infty & if \quad p = 0 \\
-\frac{1}{\lambda} ln(1-p) & if \quad p \in (0, 1]
\end{cases}
$$


**(c.Q)** Suppose that $X_1, · · · , X_n$ is an i.i.d sample from the exponential distribution with an unknown parameter $\lambda_0 > 0$. What is the maximum likelihood estimate $\hatλ_{MLE}$ for $\lambda_0$?

**(c.A)** The likelihood function $\mathbb{L}(\lambda)$ based upon the sample $X_1, · · · , X_n$ is:
$\mathbb{L}(\lambda) = \prod_{i = 1}^n f_\lambda(X_i) = \lambda ^ n e ^{-n \lambda \overline{X}}$.

Hence, we have $log\mathbb{L}(\lambda) = log (\lambda ^ n e ^{-n \lambda \overline{X}} ) = n log \lambda - n \lambda \overline{X}$, and $\frac{\partial}{\partial \lambda} = \frac{n}{\lambda} - n \overline{X}$.

Since $\overline{X} = \mathbb{E}[X] = \frac{1}{\lambda}$, we have $\frac{\partial}{\partial \lambda}\mathbb{L}(\lambda) = \frac{n}{\lambda} - n \overline{X} = 0$

So, we have the maximum likelihood estimate for $\lambda_0$ is given by $\hatλ_{MLE} = \frac{1}{\overline{X}}$.


**(d.Q)** Conduct a simulation study to explore the behavior of the maximum likelihood estimator $\hatλ_{MLE}$ for $\lambda_0$ on simulated data $X_1, · · · , X_n$ generated using the exponential distribution. Consider a setting in which $\lambda_0 = 0.01$ and generate a plot of the mean squared error as a function of the sample size. You should consider a sample sizes between $5$ and $1000$ in increments of $5$, and consider $100$ trials per sample size. For each trial of each sample size generate a random sample $X_1, · · · , X_n$ of the exponential distribution with parameter $\lambda_0 = 0.01$, then compute the maximum likelihood estimate $\hatλ_{MLE}$ for $\lambda_0$ based upon the corresponding sample. Display a plot of the mean square error of $\hatλ_{MLE}$ as an estimator for $\lambda_0$ as a function of the sample size.

**(d.A)**
```{r}
set.seed(0)
min_sample_size <- 5
max_simple_size <- 1000
sample_size_inc <- 5
num_trials_per_sample_size <- 100
lambda_0 <- 0.01

exponential_simulation_df <- crossing(trial = seq(num_trials_per_sample_size),
                                      sample_size = seq(min_sample_size, max_simple_size, sample_size_inc)) %>%
  # create data frame of all pairs of sample_size and trial
  mutate(simulation = pmap(.l = list(trial, sample_size),
                           .f = ~rexp(.y, rate = lambda_0))) %>%
  # simulate sequences of Gaussian random variables
  mutate(lambda_mle = 1 / map_dbl(.x = simulation, .f = mean)) %>%
  # compute the sample sd
  group_by(sample_size) %>%
  summarise(msq_error = mean((lambda_mle - lambda_0)^2))

exponential_simulation_df %>%
  ggplot(aes(x = sample_size, y = msq_error)) +
  geom_smooth() +
  theme_bw() +
  xlab("Sample size") + ylab("Mean square error")
```

Now download the csv file entitled “birds_data_EMATM0061” from the Assessment section within Blackboard. The csv file contains synthetic data on arrival times for birds at a bird feeder, collected over a five week period. The species of bird and their arrival time are recorded.

```{r}
file_name <- "bird_data_EMATM0061"
folder_name <- "/Users/wukexin/Desktop/SCEM/final_assessment"

birds_data_original <- read.csv(paste0(folder_name, "/", file_name, ".csv"))
```

Let’s model the sequence of time differences as independent and identically distributed random variables from an exponential distribution. More, precisely, let $Y_1, Y_2, . . . , Y_{n+1}$ denote the sequence of arrival times in seconds. Construct a new sequence of random variables $X_1, ..., X_n$ where $X_i = Y_{i+1} − Y_i$ for each $i = 1,..., n$. Model the sequence of differences in arrival times $X_1,...,X_n$ as independent and identically distributed exponential random variables.

```{r}
birds_data <- birds_data_original %>%
  mutate(time_diffs = lead(Time) - Time)

time_diffs <- birds_data %>% pull(time_diffs)
```

**(e.Q)** Compute and display the maximum likelihood estimate of the rate parameter $\hatλ_{MLE}$.

**(e.A)**
```{r}
lambda_MLE <- 1 / mean(time_diffs, na.rm = TRUE)

lambda_MLE
```

**(f.Q)** Can you give a confidence interval for $\lambda_0$ with a confidence level of $95\%$?

**(f.A)** 
```{r}
alpha <- 0.05
sample_size <- length(time_diffs)

confidence_interval_l <- qchisq(p = alpha / 2, df = 2 * sample_size)
confidence_interval_r <- qchisq(p = 1 - alpha / 2, df = 2 * sample_size)
confidence_interval <- c(confidence_interval_l / 2 / sum(time_diffs, na.rm = TRUE), confidence_interval_r / 2 / sum(time_diffs, na.rm = TRUE))
confidence_interval

```